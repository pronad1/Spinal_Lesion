{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14063335,"sourceType":"datasetVersion","datasetId":8951332}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================================================================================\n# 1. INSTALL DEPENDENCIES & IMPORTS\n# =========================================================================================\n!pip install -q timm\n\nimport os\nimport gc\nimport json\nimport random\nimport time\nimport argparse\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nfrom sklearn.metrics import roc_auc_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport timm\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# =========================================================================================\n# 2. CONFIGURATION\n# =========================================================================================\nclass Config:\n    # Model Selection\n    model_name = 'vit_base_patch16_224'  # Options: vit_small_patch16_224, vit_base_patch16_224\n    \n    # Training Parameters\n    img_size = 224\n    batch_size = 32          # Adjust based on GPU memory (32 fits T4/P100 for ViT-Base)\n    epochs = 20\n    learning_rate = 1e-4     # 0.0001\n    weight_decay = 0.01      # Regularization for ViT\n    seed = 42\n    num_workers = 2\n    \n    # Paths (Kaggle Standard Structure)\n    root_dir = Path('/kaggle/input/vindr-spinexr-modified/vindr-spinexr-a-large-annotated-medical-image-dataset')\n    train_img_dir = root_dir / 'train_png'  # Double check if folder is 'train_png' or 'train_images' in your dataset version\n    test_img_dir = root_dir / 'test_png'    # Double check if folder is 'test_png' or 'test_images'\n    train_csv = root_dir / 'annotations/train.csv'\n    test_csv = root_dir / 'annotations/test.csv'\n    \n    # Output\n    output_dir = Path('./outputs/vit_base_repro')\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create output dir\nConfig.output_dir.mkdir(parents=True, exist_ok=True)\n\n# Seeding for reproducibility\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(Config.seed)\nprint(f\"Device: {Config.device}\")\n\n# =========================================================================================\n# 3. DATASET CLASS\n# =========================================================================================\nclass SpineDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n        \n        # Pre-check existing files to avoid crashing during training\n        self.valid_images = []\n        missing = 0\n        \n        print(f\"Checking image existence for {len(df)} entries...\")\n        for idx, row in df.iterrows():\n            img_id = row['image_id']\n            # Try png first, then maybe jpg/dicom depending on dataset (assuming PNG here based on your path)\n            paths_to_try = [\n                self.img_dir / f\"{img_id}.png\",\n                self.img_dir / img_id\n            ]\n            \n            found = False\n            for p in paths_to_try:\n                if p.exists():\n                    self.valid_images.append((p, row['label']))\n                    found = True\n                    break\n            \n            if not found:\n                missing += 1\n                \n        print(f\"Verified {len(self.valid_images)} images. (Missing: {missing})\")\n\n    def __len__(self):\n        return len(self.valid_images)\n\n    def __getitem__(self, idx):\n        img_path, label = self.valid_images[idx]\n        \n        try:\n            image = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading {img_path}: {e}\")\n            # Return black image on failure to keep batch size consistent\n            image = Image.new('RGB', (Config.img_size, Config.img_size), (0, 0, 0))\n            \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, torch.tensor(label, dtype=torch.float32)\n\n# =========================================================================================\n# 4. DATA PROCESSING\n# =========================================================================================\ndef process_dataframe(csv_path):\n    df = pd.read_csv(csv_path)\n    \n    # Logic: \n    # Group by image_id. \n    # If ANY lesion_type is NOT \"No finding\", label = 1 (Abnormal).\n    # If ALL rows for an image are \"No finding\", label = 0 (Normal).\n    \n    # Helper to determine status\n    def get_status(group):\n        if (group['lesion_type'] == 'No finding').all():\n            return 0 # Normal\n        return 1 # Abnormal\n\n    # Group and map\n    image_labels = df.groupby('image_id').apply(get_status).reset_index()\n    image_labels.columns = ['image_id', 'label']\n    \n    return image_labels\n\nprint(\"Processing Train Data...\")\ntrain_df_full = process_dataframe(Config.train_csv)\n\n# Optional: Split validation from train csv if test.csv is strictly for final testing\n# Assuming we use train.csv for train/val split\ntrain_df, val_df = train_test_split(train_df_full, test_size=0.2, stratify=train_df_full['label'], random_state=Config.seed)\n\nprint(f\"Train Set: {len(train_df)} | Validation Set: {len(val_df)}\")\nprint(f\"Train Class Distribution: \\n{train_df['label'].value_counts()}\")\n\n# =========================================================================================\n# 5. TRANSFORMS & LOADERS\n# =========================================================================================\n# ViT specific transforms (ImageNet normalization)\ntrain_transforms = transforms.Compose([\n    transforms.Resize((Config.img_size, Config.img_size)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # ViT often uses 0.5 mean/std, or ImageNet\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((Config.img_size, Config.img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\n# Create Datasets\ntrain_dataset = SpineDataset(train_df, Config.train_img_dir, transform=train_transforms)\nval_dataset = SpineDataset(val_df, Config.train_img_dir, transform=val_transforms) # Using train dir for val split\n\n# Create Loaders\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=Config.batch_size, \n    shuffle=True, \n    num_workers=Config.num_workers,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=Config.batch_size, \n    shuffle=False, \n    num_workers=Config.num_workers,\n    pin_memory=True\n)\n\n# Calculate Class Weight\nnum_normal = (train_df['label'] == 0).sum()\nnum_abnormal = (train_df['label'] == 1).sum()\npos_weight = torch.tensor([num_normal / num_abnormal]).to(Config.device)\nprint(f\"Using Positive Weight: {pos_weight.item():.4f}\")\n\n# =========================================================================================\n# 6. MODEL SETUP\n# =========================================================================================\nprint(f\"Initializing {Config.model_name}...\")\ntry:\n    model = timm.create_model(Config.model_name, pretrained=True, num_classes=1)\n    print(\"Loaded pretrained weights.\")\nexcept:\n    print(\"Could not load pretrained weights. Check internet connection. Initializing random weights.\")\n    model = timm.create_model(Config.model_name, pretrained=False, num_classes=1)\n\nmodel = model.to(Config.device)\n\n# Optimization\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\noptimizer = optim.AdamW(model.parameters(), lr=Config.learning_rate, weight_decay=Config.weight_decay)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.epochs, eta_min=1e-6)\n\n# =========================================================================================\n# 7. TRAINING LOOP\n# =========================================================================================\ndef train_one_epoch(epoch_index):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch_index}/{Config.epochs} [Train]\", leave=False)\n    \n    for images, labels in pbar:\n        images, labels = images.to(Config.device), labels.to(Config.device)\n        \n        optimizer.zero_grad()\n        outputs = model(images).squeeze(1) # Ensure shape [batch] not [batch, 1]\n        \n        # Handle case where batch size is 1 (squeeze removes batch dim)\n        if outputs.ndim == 0: outputs = outputs.unsqueeze(0)\n            \n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        preds = (torch.sigmoid(outputs) > 0.5).float()\n        total += labels.size(0)\n        correct += (preds == labels).sum().item()\n        \n        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n        \n    return running_loss / len(train_loader), correct / total\n\ndef validate_one_epoch(epoch_index):\n    model.eval()\n    running_loss = 0.0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    pbar = tqdm(val_loader, desc=f\"Epoch {epoch_index}/{Config.epochs} [Val]\", leave=False)\n    \n    with torch.no_grad():\n        for images, labels in pbar:\n            images, labels = images.to(Config.device), labels.to(Config.device)\n            \n            outputs = model(images).squeeze(1)\n            if outputs.ndim == 0: outputs = outputs.unsqueeze(0)\n                \n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            probs = torch.sigmoid(outputs)\n            preds = (probs > 0.5).float()\n            \n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n            \n    # Calculate Metrics\n    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n    try:\n        auroc = roc_auc_score(all_labels, all_probs)\n    except:\n        auroc = 0.5 # Fallback if only one class exists in batch\n        \n    return running_loss / len(val_loader), accuracy, auroc, all_labels, all_probs\n\n# =========================================================================================\n# 8. EXECUTION\n# =========================================================================================\nbest_auroc = 0.0\nhistory = []\n\nprint(f\"\\nStarting training for {Config.epochs} epochs...\")\n\nfor epoch in range(1, Config.epochs + 1):\n    # Train\n    train_loss, train_acc = train_one_epoch(epoch)\n    \n    # Val\n    val_loss, val_acc, val_auroc, _, _ = validate_one_epoch(epoch)\n    \n    # Scheduler\n    scheduler.step()\n    curr_lr = optimizer.param_groups[0]['lr']\n    \n    # Logging\n    print(f\"Epoch {epoch}: \"\n          f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | \"\n          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | AUROC: {val_auroc:.4f} | LR: {curr_lr:.6f}\")\n    \n    # Save checkpoint if best\n    if val_auroc > best_auroc:\n        best_auroc = val_auroc\n        torch.save(model.state_dict(), Config.output_dir / \"best_model.pth\")\n        print(f\"--> New Best AUROC! Saved model.\")\n        \n    # Save History\n    history.append([epoch, train_loss, train_acc, val_loss, val_acc, val_auroc])\n\n# =========================================================================================\n# 9. FINAL EVALUATION & METRICS\n# =========================================================================================\nprint(\"\\nRunning Final Evaluation on Best Model...\")\n\n# Load best weights\nmodel.load_state_dict(torch.load(Config.output_dir / \"best_model.pth\"))\nmodel.eval()\n\n# Re-run validation logic on validation set to get final metrics\n_, final_acc, final_auroc, y_true, y_probs = validate_one_epoch(\"Final\")\ny_pred = (np.array(y_probs) > 0.5).astype(int)\n\n# Metrics with Bootstrap CI\ndef bootstrap_metric(y_true, y_probs, metric_fn, n_boot=1000):\n    scores = []\n    rng = np.random.RandomState(Config.seed)\n    indices = np.arange(len(y_true))\n    \n    for _ in range(n_boot):\n        idx = rng.choice(indices, len(indices), replace=True)\n        try:\n            score = metric_fn(np.array(y_true)[idx], np.array(y_probs)[idx])\n            scores.append(score)\n        except:\n            pass # Handle single-class batches\n    \n    return np.percentile(scores, 2.5), np.percentile(scores, 97.5)\n\n# Calculate final metrics\ntn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\nsensitivity = tp / (tp + fn)\nspecificity = tn / (tn + fp)\nf1 = f1_score(y_true, y_pred)\n\n# Get CIs\nauroc_ci = bootstrap_metric(y_true, y_probs, roc_auc_score)\n# For F1, Sensitivity, Specificity, we need binary preds\n# Note: Simplified CI calculation for brevity\nsens_ci = (0.0, 0.0) # Placeholder or implement full bootstrap loop as needed\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINAL RESULTS - ViT Base Patch16 224\")\nprint(\"=\"*50)\nprint(f\"AUROC       : {final_auroc*100:.2f}% (CI: {auroc_ci[0]*100:.1f}-{auroc_ci[1]*100:.1f})\")\nprint(f\"F1 Score    : {f1*100:.2f}%\")\nprint(f\"Sensitivity : {sensitivity*100:.2f}%\")\nprint(f\"Specificity : {specificity*100:.2f}%\")\nprint(\"-\" * 50)\nprint(\"Confusion Matrix:\")\nprint(f\"TN: {tn} | FP: {fp}\")\nprint(f\"FN: {fn} | TP: {tp}\")\nprint(\"=\"*50)\n\n# Save history\nhist_df = pd.DataFrame(history, columns=['Epoch', 'Train_Loss', 'Train_Acc', 'Val_Loss', 'Val_Acc', 'Val_AUROC'])\nhist_df.to_csv(Config.output_dir / 'training_log.csv', index=False)\nprint(\"Saved training log.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T17:06:30.876578Z","iopub.execute_input":"2025-12-11T17:06:30.877242Z","iopub.status.idle":"2025-12-11T18:42:04.269153Z","shell.execute_reply.started":"2025-12-11T17:06:30.877208Z","shell.execute_reply":"2025-12-11T18:42:04.268304Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\nProcessing Train Data...\nTrain Set: 6711 | Validation Set: 1678\nTrain Class Distribution: \nlabel\n0    3408\n1    3303\nName: count, dtype: int64\nChecking image existence for 6711 entries...\nVerified 6527 images. (Missing: 184)\nChecking image existence for 1678 entries...\nVerified 1635 images. (Missing: 43)\nUsing Positive Weight: 1.0318\nInitializing vit_base_patch16_224...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9530272d53b4e1284084d5f50e2822f"}},"metadata":{}},{"name":"stdout","text":"Loaded pretrained weights.\n\nStarting training for 20 epochs...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1: Train Loss: 0.6203 | Acc: 0.6637 | Val Loss: 0.5583 | Val Acc: 0.7235 | AUROC: 0.7970 | LR: 0.000099\n--> New Best AUROC! Saved model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2: Train Loss: 0.5892 | Acc: 0.6832 | Val Loss: 0.5436 | Val Acc: 0.7309 | AUROC: 0.8187 | LR: 0.000098\n--> New Best AUROC! Saved model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3: Train Loss: 0.5427 | Acc: 0.7199 | Val Loss: 0.5106 | Val Acc: 0.7560 | AUROC: 0.8356 | LR: 0.000095\n--> New Best AUROC! Saved model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 4/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4: Train Loss: 0.5265 | Acc: 0.7271 | Val Loss: 0.4957 | Val Acc: 0.7700 | AUROC: 0.8503 | LR: 0.000091\n--> New Best AUROC! Saved model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 5/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5: Train Loss: 0.4965 | Acc: 0.7559 | Val Loss: 0.4840 | Val Acc: 0.7670 | AUROC: 0.8567 | LR: 0.000086\n--> New Best AUROC! Saved model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 6/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6: Train Loss: 0.4806 | Acc: 0.7660 | Val Loss: 0.4856 | Val Acc: 0.7737 | AUROC: 0.8553 | LR: 0.000080\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 7/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7: Train Loss: 0.4568 | Acc: 0.7817 | Val Loss: 0.5295 | Val Acc: 0.7737 | AUROC: 0.8673 | LR: 0.000073\n--> New Best AUROC! Saved model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 8/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8: Train Loss: 0.4383 | Acc: 0.7981 | Val Loss: 0.5461 | Val Acc: 0.7462 | AUROC: 0.8648 | LR: 0.000066\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 9/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9: Train Loss: 0.4255 | Acc: 0.8053 | Val Loss: 0.4964 | Val Acc: 0.7688 | AUROC: 0.8559 | LR: 0.000058\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 10/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10: Train Loss: 0.3961 | Acc: 0.8186 | Val Loss: 0.5079 | Val Acc: 0.7792 | AUROC: 0.8617 | LR: 0.000051\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 11/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11: Train Loss: 0.3626 | Acc: 0.8373 | Val Loss: 0.4915 | Val Acc: 0.7829 | AUROC: 0.8681 | LR: 0.000043\n--> New Best AUROC! Saved model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 12/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12: Train Loss: 0.3279 | Acc: 0.8545 | Val Loss: 0.4942 | Val Acc: 0.7835 | AUROC: 0.8666 | LR: 0.000035\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 13/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13: Train Loss: 0.2887 | Acc: 0.8794 | Val Loss: 0.5289 | Val Acc: 0.7817 | AUROC: 0.8629 | LR: 0.000028\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 14/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14: Train Loss: 0.2442 | Acc: 0.8993 | Val Loss: 0.5923 | Val Acc: 0.7645 | AUROC: 0.8487 | LR: 0.000021\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 15/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15: Train Loss: 0.2081 | Acc: 0.9165 | Val Loss: 0.5569 | Val Acc: 0.7835 | AUROC: 0.8666 | LR: 0.000015\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 16/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8b55fa8473f43eba134d4cb88cd0d26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 16/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11864739bb43439b84969584e32f19fa"}},"metadata":{}},{"name":"stdout","text":"Epoch 16: Train Loss: 0.1640 | Acc: 0.9346 | Val Loss: 0.6658 | Val Acc: 0.7817 | AUROC: 0.8593 | LR: 0.000010\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 17/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d179bb7b3fa24d2bab4ad480aaceacf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 17/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a69eb36cb21e48f2b543f7ac7cf032e6"}},"metadata":{}},{"name":"stdout","text":"Epoch 17: Train Loss: 0.1321 | Acc: 0.9493 | Val Loss: 0.7597 | Val Acc: 0.7884 | AUROC: 0.8596 | LR: 0.000006\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 18/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 18/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 18: Train Loss: 0.1060 | Acc: 0.9631 | Val Loss: 0.7546 | Val Acc: 0.7835 | AUROC: 0.8639 | LR: 0.000003\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 19/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 19/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 19: Train Loss: 0.0942 | Acc: 0.9660 | Val Loss: 0.7988 | Val Acc: 0.7859 | AUROC: 0.8635 | LR: 0.000002\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 20/20 [Train]:   0%|          | 0/204 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 20/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 20: Train Loss: 0.0858 | Acc: 0.9678 | Val Loss: 0.8233 | Val Acc: 0.7823 | AUROC: 0.8638 | LR: 0.000001\n\nRunning Final Evaluation on Best Model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch Final/20 [Val]:   0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n==================================================\nFINAL RESULTS - ViT Base Patch16 224\n==================================================\nAUROC       : 86.81% (CI: 85.0-88.3)\nF1 Score    : 78.05%\nSensitivity : 78.09%\nSpecificity : 78.48%\n--------------------------------------------------\nConfusion Matrix:\nTN: 649 | FP: 178\nFN: 177 | TP: 631\n==================================================\nSaved training log.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\nfrom IPython.display import FileLink\n\n# ==========================================\n# SAFEGUARD: SAVE & DOWNLOAD\n# ==========================================\n\n# 1. Define output paths\nOUTPUT_DIR = Path('./outputs/vit_base_repro')\nARCHIVE_NAME = 'vit_model_results'\n\nprint(f\"Compressing results from {OUTPUT_DIR}...\")\n\n# 2. Create a ZIP file of the output directory\n# This ensures you get the model weights, logs, and prediction CSVs\nshutil.make_archive(ARCHIVE_NAME, 'zip', OUTPUT_DIR)\n\nprint(f\"Compression complete: {ARCHIVE_NAME}.zip\")\n\n# 3. Generate Download Link\n# Click this link immediately after training finishes!\nprint(f\"\\n⬇️ CLICK HERE TO DOWNLOAD RESULTS ⬇️\")\ndisplay(FileLink(f'{ARCHIVE_NAME}.zip'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:42:04.274008Z","iopub.execute_input":"2025-12-11T18:42:04.274258Z","iopub.status.idle":"2025-12-11T18:42:20.908368Z","shell.execute_reply.started":"2025-12-11T18:42:04.274233Z","shell.execute_reply":"2025-12-11T18:42:20.907655Z"}},"outputs":[{"name":"stdout","text":"Compressing results from outputs/vit_base_repro...\nCompression complete: vit_model_results.zip\n\n⬇️ CLICK HERE TO DOWNLOAD RESULTS ⬇️\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/vit_model_results.zip","text/html":"<a href='vit_model_results.zip' target='_blank'>vit_model_results.zip</a><br>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}