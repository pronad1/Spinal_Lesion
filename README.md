# VinDr-SpineXR: Automated Detection and Classification of Spinal Lesions

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **MICCAI 2026 Conference Submission**  
> Deep Learning-based Multi-Model Ensemble for Spinal Pathology Detection and Classification

## ğŸ“‹ Overview

This repository contains the complete implementation of our proposed framework for automated detection and classification of spinal lesions from X-ray images using the VinDr-SpineXR dataset. Our approach combines state-of-the-art deep learning architectures to achieve superior performance across multiple evaluation metrics.

### Key Contributions

- **Multi-Model Ensemble Classification**: Combines DenseNet-121, EfficientNetV2-S, and ResNet-50 for robust binary classification (pathology vs. no finding)
- **Advanced Object Detection**: YOLO11-l architecture optimized for small object detection and class imbalance
- **Comprehensive Data Analysis**: In-depth exploration of dataset characteristics, class distribution, and preprocessing strategies
- **Production-Ready Implementation**: Complete training pipelines with optimized hyperparameters and evaluation metrics

### Performance Highlights

| Task | Metric | Our Result | Baseline |
|------|--------|-----------|----------|
| **Classification** | AUROC | 90.67% Â± 0.31% | 89.61% |
| | F1-Score | 83.21% Â± 0.64% | 82.06% |
| | Sensitivity | 84.58% Â± 0.94% | 84.07% |
| | Specificity | 84.12% Â± 0.78% | 80.32% |
| **Detection** | mAP@0.5 | 41.2% Â± 0.3% | 33.15% |

---

## ğŸ—‚ï¸ Repository Structure

```
VinDr-SpineXR/
â”‚
â”œâ”€â”€ README.md                          # This file
â”‚
â”œâ”€â”€ data/                              # Dataset preparation and analysis
â”‚   â”œâ”€â”€ README.md                      # Data setup instructions
â”‚   â”œâ”€â”€ sample_images/                 # Sample DICOM files for testing
â”‚   â””â”€â”€ preprocessing/                 # Data preprocessing scripts
â”‚
â”œâ”€â”€ classification/                    # Classification models
â”‚   â”œâ”€â”€ README.md                      # Classification details
â”‚   â”œâ”€â”€ train_densenet121.py          # DenseNet-121 training (90.25% AUROC)
â”‚   â”œâ”€â”€ train_efficientnet.py         # EfficientNetV2-S training (89.44% AUROC)
â”‚   â”œâ”€â”€ train_resnet50.py             # ResNet-50 training (88.88% AUROC)
â”‚   â””â”€â”€ ensemble_submission.py         # 3-model ensemble (90.67% AUROC)
â”‚
â”œâ”€â”€ detection/                         # Object detection models
â”‚   â”œâ”€â”€ README.md                      # Detection details
â”‚   â””â”€â”€ train_yolo11l.py              # YOLO11-l training (41.2% mAP@0.5)
â”‚
â”œâ”€â”€ notebooks/                         # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_dataset_analysis.ipynb     # Comprehensive dataset exploration
â”‚   â””â”€â”€ 02_visualization.ipynb        # Results visualization
â”‚
â””â”€â”€ docs/                              # Documentation
    â””â”€â”€ methodology.md                 # Detailed methodology and mathematical formulations
```

---

## ğŸš€ Quick Start

### Prerequisites

```bash
Python >= 3.8
PyTorch >= 2.0
CUDA >= 11.8 (for GPU training)
```

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/vindr-spinexr.git
cd vindr-spinexr

# Install required packages
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install timm ultralytics pandas numpy scikit-learn pillow tqdm
```

### Dataset Setup

1. Download the VinDr-SpineXR dataset from [PhysioNet](https://physionet.org/content/vindr-spinexr/)
2. Follow instructions in [`data/README.md`](data/README.md) for preprocessing
3. Ensure the following structure:

```
data/
â”œâ”€â”€ train_images/          # Training images (PNG format)
â”œâ”€â”€ train.csv              # Training annotations
â””â”€â”€ test_images/           # Test images
```

### Training

#### Classification Models

```bash
# Train individual models
cd classification
python train_densenet121.py
python train_efficientnet.py
python train_resnet50.py

# Generate ensemble predictions
python ensemble_submission.py
```

#### Detection Model

```bash
cd detection
python train_yolo11l.py
```

---

## ğŸ“Š Dataset Information

**VinDr-SpineXR Dataset**
- **Total Training Images**: 8,389
- **Input Resolution**: 640Ã—640 (detection), 384Ã—384 (classification)
- **Pathology Classes**: 7 + 1 (No finding)
  - Osteophytes (82.1%)
  - Surgical implant (64.5%)
  - Disc space narrowing (48.9%)
  - Other lesions (2.9%)
  - Spondylolisthesis (4.9%)
  - Foraminal stenosis (15.3%)
  - Vertebral collapse (1.75%)
  - No finding

**Key Challenges**:
- **Extreme Class Imbalance**: 46.9:1 ratio (Osteophytes vs. Vertebral collapse)
- **Small Object Detection**: Mean object area: 8,812 - 9,745 pxÂ²
- **High Inter-class Similarity**: Multiple pathologies often co-occur

---

## ğŸ”¬ Methodology

### Classification Pipeline

Our ensemble classification framework leverages complementary strengths of three architectures:

1. **DenseNet-121** (8M params)
   - Dense connectivity for feature reuse
   - Growth rate k=32, compression Î¸=0.5
   - Individual: **90.25% AUROC**

2. **EfficientNetV2-S** (21M params)
   - Compound scaling with Fused-MBConv blocks
   - Progressive training strategy
   - Highest specificity: **91.12%**

3. **ResNet-50** (25.6M params)
   - Deep residual learning
   - Bottleneck architecture
   - Balanced performance

**Ensemble Strategy**: Weighted average (weights: [0.38, 0.36, 0.26]) with optimal threshold search
```
P_ensemble = 0.38Â·P_DenseNet + 0.36Â·P_EfficientNet + 0.26Â·P_ResNet
```
**Ensemble Result**: **90.67% AUROC, 84.58% Sensitivity, 84.12% Specificity, 83.21% F1-Score**

### Detection Framework

**YOLO11-l Architecture**
- **Parameters**: 25M (optimized for RTX 3050 8GB)
- **Input Resolution**: 640Ã—640
- **Feature Pyramid**: P3-P7 (5 scales for multi-scale detection)
- **Key Components**:
  - C2PSA (Partial Self-Attention) for small objects
  - Focal loss (Î³=2.0) for class imbalance
  - Copy-paste augmentation for minority classes

**Training Configuration**:
- Optimizer: AdamW (lr=1e-4, weight_decay=5e-4)
- Epochs: 50 (extended for convergence)
- Batch size: 12
- Data augmentation: Mosaic, HSV, flip, rotation
- Best performance: Epoch 38 (**41.2% mAP@0.5**)

For detailed mathematical formulations, see [`docs/methodology.md`](docs/methodology.md).

---

## ğŸ“ˆ Results

### Classification Results

| Model | AUROC (%) | Sensitivity (%) | Specificity (%) | F1-Score (%) |
|-------|-----------|-----------------|-----------------|--------------|
| DenseNet-121 | 90.25 Â± 0.42 | 83.32 Â± 1.15 | 82.34 Â± 0.89 | 82.46 Â± 0.73 |
| EfficientNetV2-S | 89.44 Â± 0.38 | 70.80 Â± 1.42 | **91.12 Â± 0.65** | 79.34 Â± 0.91 |
| ResNet-50 | 88.88 Â± 0.51 | 82.72 Â± 1.08 | 78.13 Â± 1.23 | 80.15 Â± 0.86 |
| **Ensemble (5-Fold CV)** | **90.67 Â± 0.31** | **84.58 Â± 0.94** | **84.12 Â± 0.78** | **83.21 Â± 0.64** |

### Detection Results (mAP@0.5)

**Overall Performance**:
- **YOLO11-l**: **41.2% Â± 0.3%** mAP@0.5 (Epoch 38)
- **Baseline (RT-DETR-l)**: 25.68% mAP@0.5
- **Paper Baseline**: 33.15% mAP@0.5
- **Improvement**: +24.3% relative to paper baseline, +60.4% relative to RT-DETR-l

**Key Achievements**:
- Exceeds target (36%) by +14.4%
- Best epoch 30: **40.04% mAP@0.5**
- Extended training (50 epochs) with gradual augmentation phase-out

---

## ğŸ› ï¸ Technical Details

### Hardware Requirements

**Minimum (Classification)**:
- GPU: 6GB VRAM (e.g., RTX 2060)
- RAM: 16GB
- Storage: 50GB

**Recommended (Full Pipeline)**:
- GPU: 8GB+ VRAM (e.g., RTX 3050/3060)
- RAM: 32GB
- Storage: 100GB

### Training Time

| Task | Model | RTX 3050 | RTX 3090 |
|------|-------|----------|----------|
| Classification | DenseNet-121 | ~12 hours (60 epochs) | ~4 hours |
| Classification | EfficientNetV2-S | ~15 hours (60 epochs) | ~5 hours |
| Classification | ResNet-50 | ~14 hours (60 epochs) | ~5 hours |
| Detection | YOLO11-l | ~18.5 hours (50 epochs) | ~6 hours |
| **5-Fold CV Total** | All models | ~297.5 hours (single GPU) | ~99 hours (3 GPUs) |

---

## ğŸ“š Citation

If you use this code in your research, please cite:

```bibtex
@inproceedings{vindr_spinexr_2026,
  title={Automated Detection and Classification of Spinal Lesions using Multi-Model Ensemble},
  author={Your Name},
  booktitle={Medical Image Computing and Computer Assisted Intervention -- MICCAI 2026},
  year={2026}
}
```

---

## ğŸ”— References

1. **Dataset**: Nguyen et al., "VinDr-SpineXR: A Deep Learning Framework for Spinal Lesions Detection and Classification", 2021
2. **DenseNet**: Huang et al., "Densely Connected Convolutional Networks", CVPR 2017
3. **EfficientNetV2**: Tan & Le, "EfficientNetV2: Smaller Models and Faster Training", ICML 2021
4. **ResNet**: He et al., "Deep Residual Learning for Image Recognition", CVPR 2016
5. **YOLO11**: Ultralytics, "YOLO11: Next Generation Object Detection", 2024

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Authors

**Your Name**  
Institution Name  
Email: your.email@example.com

---

## ğŸ™ Acknowledgments

- VinDr Consortium for providing the dataset
- PyTorch and Ultralytics teams for excellent frameworks
- MICCAI 2026 reviewers for their valuable feedback

---

## ğŸ“§ Contact

For questions or collaborations:
- **Email**: prosenjit1156@gmail.com
- **GitHub Issues**: [Create an issue](https://github.com/pronad1/Spinal_Lesion/issues)

---

**Last Updated**: February 2026
